-- ds.page: Storage and Retrieval

**Note**: The content is taken from [`Designing Data Intensive Application by
Martin Kleppmann`](https://public.nikhil.io/Designing%20Data%20Intensive%20Applications.pdf)




-- ds.h1: Data Structures That Power Your Database

Consider the world’s simplest database, implemented as two Bash functions:

-- ds.code:
lang: sh

#!/bin/bash
db_set () {
 echo "$1,$2" >> database
}
db_get () {
 grep "^$1," database | sed -e "s/^$1,//" | tail -n 1
}

$ db_set 42 '{"name":"San Francisco","attractions":["Golden Gate Bridge"]}'
$ db_get 42
{"name":"San Francisco","attractions":["Golden Gate Bridge"]}

$ db_set 42 '{"name":"San Francisco","attractions":["Exploratorium"]}'
$ db_get 42
{"name":"San Francisco","attractions":["Exploratorium"]}

$ cat database
42,{"name":"San Francisco","attractions":["Golden Gate Bridge"]}
42,{"name":"San Francisco","attractions":["Exploratorium"]}

-- ds.markdown:

Our db_set function actually has pretty good performance for something that is so
simple, because appending to a file is generally very efficient. Similarly to what
db_set does, many databases internally use a log, which is an append-only data file.

On the other hand, our db_get function has terrible performance if you have a large
number of records in your database. Every time you want to look up a key, db_get
has to scan the entire database file from beginning to end, looking for occurrences of
the key. In algorithmic terms, the cost of a lookup is O(n): if you double the number
of records n in your database, a lookup takes twice as long. That’s not good.

In order to efficiently find the value for a particular key in the database, we need a
different data structure: an *index*.

An index is an additional structure that is derived from the primary data.
Maintaining additional structures incurs overhead, especially on writes. For
writes, it’s hard to beat the performance of simply appending to a file, because
that’s the simplest possible write operation. Any kind of index usually slows down
writes, because the index also needs to be updated every time data is written.

This is an important trade-off in storage systems: well-chosen indexes speed up read
queries, but every index slows down writes. For this reason, databases don’t usually
index everything by default, but require you—the application developer or database
administrator—to choose indexes manually, using your knowledge of the
application’s typical query patterns.





-- ds.h2: Hash Indexes

Let’s say our data storage consists only of appending to a file, as in the preceding
example. Then the simplest possible indexing strategy is this: keep an in-memory
hash map where every key is mapped to a byte offset in the data file—the location at
which the value can be found.

This may sound simplistic, but it is a viable approach. In fact, this is essentially what
Bitcask (the default storage engine in Riak) does. Bitcask offers high-performance
reads and writes, subject to the requirement that all the keys fit in the available RAM,
since the hash map is kept completely in memory.

A storage engine like Bitcask is well suited to situations where the value for each key
is updated frequently. For example, the key might be the URL of a cat video, and the
value might be the number of times it has been played (incremented every time
someone hits the play button). In this kind of workload, there are a lot of writes, but
there are not too many distinct keys—you have a large number of writes per key, but
it’s feasible to keep all keys in memory.

*So how do we avoid eventually running out of disk space?*
A good solution is to break the log into segments of a certain size by closing a
segment file when it reaches a certain size, and making subsequent writes to a
new segment file. We can then perform compaction on these segment. Compaction
means throwing away duplicate keys in the log, and keeping only the most recent
update for each key.

-- ds.code:
lang: txt

\// Data file segment

mew: 1078 | purr: 2108 | purr: 2304 | mew: 409 | yawn: 609 | yawn: 7809 | purr: 908

\// Compaction

yawn: 7809 | purr: 908 | mew: 409

-- ds.markdown:

Since compaction often makes segments much smaller, we can also merge several
segments together at the same time as performing the compaction.

Each segment now has its own in-memory hash table, mapping keys to file offsets. In
order to find the value for a key, we first check the most recent segment’s hash map;
if the key is not present we check the second-most-recent segment, and so on. The
merging process keeps the number of segments small, so lookups don’t need to check
many hash maps.


-- ds.h3: Implementation

Some of the issues that are important in a real implementation are:

- *File format*:

CSV is not the best format for a log. It’s faster and simpler to use a binary format
that first encodes the length of a string in bytes, followed by the raw string

- *Deleting records*:

If you want to delete a key and its associated value, you have to append a special
deletion record to the data file (sometimes called a tombstone). When log
segments are merged, the tombstone tells the merging process to discard any
previous values for the deleted key.

- *Crash recovery*:

Can read the entire segment file from beginning to end and noting the offset of
the most recent value for every key, but then time consuming process.
Bitcask speeds up recovery by storing a snapshot of each segment’s hash map on
disk, which can be loaded into memory more quickly.

- *Partially written records*:

Handle by checking checksum of the record.

- *Concurrency control*:

As writes are appended to the log in a strictly sequential order, a common
implementation choice is to have only one writer thread. Data file segments are
append-only and otherwise immutable, so they can be read concurrently by
multiple threads.


-- ds.h3: Advantage of append-only approach

An append-only log seems wasteful at first glance: why don’t you update the file in
place, overwriting the old value with the new value? But an append-only design turns
out to be good for several reasons:


- Appending and segment merging are sequential write operations, which are
generally much faster than random writes, especially on magnetic spinning-disk
hard drives.

- Concurrency and crash recovery are much simpler if segment files are append
only or immutable. For example, you don’t have to worry about the case where a
crash happened while a value was being overwritten, leaving you with a file
containing part of the old and part of the new value spliced together.


-- ds.h3: Limitation

However, the hash table index also has limitations:

- The hash table must fit in memory, so if you have a very large number of keys,
you’re out of luck.

- Range queries are not efficient. For example, you cannot easily scan over all
keys between kitty00000 and kitty99999—you’d have to look up each key
individually in the hash maps.




-- ds.h2: SSTables and LSM-Trees

The sequence of key-value pairs is sorted by key in our segment files.
We call this format Sorted String Table, or SSTable for short. We also require
that each key only appears once within each merged segment file.

-- ds.h3: Advantage:

- Merging segments is simple and efficient, even if the files are bigger than the
available memory. The approach is like the one used in the mergesort algorithm,
you start reading the input files side by side, look at the first key in each
file, copy the lowest key (according to the sort order) to the output file, and
repeat. This produces a new merged segment file, also sorted by key.

- In order to find a particular key in the file, you no longer need to keep an
index of all the keys in memory. It's sorted so you'll get the key offset from
key's present in the memory. In-memory key can be sparse: one key for every few
kilobytes of segment file is sufficient.

- It is possible to group those records into a block and compress it before
writing it to disk. Each entry of the sparse in-memory index then points at the
start of a compressed block.

-- ds.h3: Constructing and maintaining SSTables

Maintaining a sorted structure on disk is possible but maintaining it in memory
is much easier. There are plenty of well-known tree data structures that you can
use, such as red-black trees or AVL trees. With these data structures, you can
insert keys in any order and read them back in sorted order.

- In-memory balanced tree (eg red-black tree) is called a `memtable`.
- When the memtable gets bigger than some threshold—typically a few megabytes
  write it out to disk as an SSTable file where key-value pairs sorted by key.
  The new SSTable file becomes the most recent segment of the database.
- In order to serve a read request, first try to find the key in the memtable, then in
  the most recent on-disk segment, then in the next-older segment, etc
- Run a merging and compaction process in the background

In case of crash, the memtable can be restored by keeping a separate log on disk.


-- ds.h3: Making an LSM-tree out of SSTables

Storage engines that are based on this principle of merging and compacting sorted
files are often called LSM (Log-Structured Merge-Tree) storage engines.

Lucene, an indexing engine for full-text search used by Elasticsearch and Solr, uses a
similar method for storing its term dictionary [12, 13]. A full-text index is much more
complex than a key-value index but is based on a similar idea: given a word in a
search query, find all the documents (web pages, product descriptions, etc.) that
mention the word. This is implemented with a key-value structure where the key is a
word (a term) and the value is the list of IDs of all the documents that contain the
word (the postings list).


-- ds.h3: Performance optimizations

In order to optimize access, storage engines often use additional *Bloom filters*.
A Bloom filter is a memory-efficient data structure for approximating the contents
of a set. It can tell you if a key does not appear in the database, and thus
saves many unnecessary disk reads for nonexistent keys



-- ds.h2: B-Trees

B-trees break the database down into fixed-size blocks or pages, traditionally
4 KB in size (sometimes bigger), and read or write one page at a time. This
design corresponds more closely to the underlying hardware, as disks are also arranged
in fixed-size blocks.

Each page can be identified using an address or location, which allows one page to
refer to another on disk. We can use these page references to construct a tree of pages

-- ds.code:
lang: txt

Look up for user_id 251

[100, **200**, 300, 400, ...]
         |
[200, 210, 220, 230, 240, **250**,...]
                             |
[250, **251**, 252, 253, 254, ...]

-- ds.markdown:

This algorithm ensures that the tree remains balanced: a B-tree with n keys always
has a depth of O(log n). Most databases can fit into a B-tree that is three or four levels
deep, so you don’t need to follow many page references to find the page you are
looking for. (A four-level tree of 4 KB pages with a branching factor of 500 can
store up to 256 TB.)

In order to make the database resilient to crashes, it is common for B-tree
implementations to include an additional data structure on disk: a write-ahead
log (WAL, also known as a redo log). This is an append-only file to which every
B-tree modification must be written before it can be applied to the pages of the
tree itself.


-- ds.h2: Other Indexing Structures

A secondary index can easily be constructed from a key-value index. The main
difference is that keys are not unique. This can be solved in two ways: either
by making each value in the index a list of matching row identifiers or by making
each key unique by appending a row identifier to it.

-- ds.h3: Storing values within the index

The key in an index is the thing that queries search for, but the value can be one of
two things: it could be the actual row (document, vertex) in question, or it could be a
reference to the row stored elsewhere. In the latter case, the place where rows are
stored is known as a heap file, and it stores data in no particular order (it may be
append-only, or it may keep track of deleted rows in order to overwrite them with
new data later). The heap file approach is common because it avoids duplicating data
when multiple secondary indexes are present.

It can be desirable to store the indexed row directly within an index. This is
known as a **clustered index**. For example, in MySQL’s InnoDB storage engine,
the primary key of a table is always a clustered index, and secondary indexes
refer to the primary key (rather than a heap file location).

A compromise between a *clustered index* (storing all row data within the index) and
a *nonclustered index* (storing only references to the data within the index) is known
as a covering index or index with included columns, which stores some of a table’s
columns within the index. This allows some queries to be answered by using the
index alone (in which case, the index is said to cover the query).

-- ds.h3: Multi-column indexes

The most common type of multi-column index is called a concatenated index, which
simply combines several fields into one key by appending one column to another.
This is like an old-fashioned paper phone book, which provides an index from
(lastname, firstname) to phone number. Due to the sort order, the index can be
used to find all the people with a particular last name, or all the people with
a particular lastname-firstname combination. However, the index is useless if
you want to find all the people with a particular first name.

A restaurant search website may have a database containing the latitude and
longitude of each restaurant. When a user is looking at the restaurants on a map
requires a two-dimensional range query like the following:

-- ds.code:
lang: sql

SELECT * FROM restaurants WHERE latitude > 51.4946 AND latitude < 51.5079
 AND longitude > -0.1162 AND longitude < -0.1004;

-- ds.markdown:

One option is to translate a two-dimensional location into a single number using a
space-filling curve, and then to use a regular B-tree index [34]. More commonly,
specialized spatial indexes such as *R-trees* are used.


-- ds.h3: Full-text search and fuzzy indexes

Full-text search engines commonly allow a search for one word to be expanded to
include synonyms of the word, to ignore grammatical variations of words, and to
search for occurrences of words near each other in the same document, and support
various other features that depend on linguistic analysis of the text.
Lucene is able to search text for words within a certain edit distance (an edit
distance of 1 means that one letter has been added, removed, or replaced)

*Lucene* uses a SSTable-like structure for its term dictionary. In LevelDB, this
in-memory index is a sparse collection of some of the keys, but in Lucene, the
in-memory index is a finite state automaton over the characters in the keys,
similar to a *trie*.









-- ds.h1: Transaction Processing or Analytics?

**Online transaction processing (OLTP)** looks up a small number of records by some
key, using an index. Records are inserted or updated based on the user’s input.

**Online analytic processing (OLAP)** are analytic queries that are often written
by business analysts, and feed into reports that help the management of a company
make better decisions (business intelligence)

-- ds.h2: Data Warehousing

A data warehouse, by contrast, is a separate database that analysts can query to
their hearts’ content, without affecting OLTP operations. The data warehouse
contains a read-only copy of the data in all the various OLTP systems in the
company. Data is extracted from OLTP databases (using either a periodic data dump
or a continuous stream of updates), transformed into an analysis-friendly schema,
cleaned up, and then loaded into the data warehouse. This process of getting data
into the warehouse is known as **Extract–Transform–Load (ETL)**.

-- ds.image:
src: $assets.files.ddd.data-intensive-application.images.3-1.png

-- ds.h3: The divergence between OLTP databases and data warehouses

On the surface, a data warehouse and a relational OLTP database look similar,
because they both have a SQL query interface. However, the internals of the systems
can look quite different, because they are optimized for very different query patterns.
Many database vendors now focus on supporting either transaction processing or
analytics workloads, but not both.


-- ds.h2: Stars and Snowflakes: Schemas for Analytics

Many data warehouses are used in a fairly formulaic style, known as a star schema
(also known as dimensional modeling).

The example schema shows a data warehouse that might be found at a grocery retailer.
At the center of the schema is a so-called fact table (in this example, it is
called **fact_sales**). Each row of the fact table represents an event that
occurred at a particular time (here, each row represents a customer’s purchase
of a product).

-- ds.image:
src: $assets.files.ddd.data-intensive-application.images.3-2.png

-- ds.markdown:

Usually, facts are captured as individual events, because this allows maximum
flexibility of analysis later. However, this means that the fact table can become
extremely large.

The name “star schema” comes from the fact that when the table relationships are
visualized, the fact table is in the middle, surrounded by its dimension tables; the
connections to these tables are like the rays of a star.

A variation of this template is known as the **snowflake schema**, where
dimensions are further broken down into subdimensions. For example, there could
be separate tables for brands and product categories, and each row in the
`dim_product` table could reference the brand and category as foreign keys,
rather than storing them as strings in the `dim_product` table. Snowflake
schemas are more normalized than star schemas, but star schemas are often
preferred because they are simpler for analysts to work with


-- ds.h2: Column-Oriented Storage

Although fact tables are often over 100 columns wide, a typical data warehouse
query only accesses 4 or 5 of them at one time.

-- ds.code:
lang: sql

SELECT
 dim_date.weekday, dim_product.category,
 SUM(fact_sales.quantity) AS quantity_sold
FROM fact_sales
 JOIN dim_date ON fact_sales.date_key = dim_date.date_key
 JOIN dim_product ON fact_sales.product_sk = dim_product.product_sk
WHERE
 dim_date.year = 2013 AND
 dim_product.category IN ('Fresh fruit', 'Candy')
GROUP BY
 dim_date.weekday, dim_product.category;

-- ds.markdown:

The query only needs to access three columns of the fact_sales table: `date_key`,
`product_sk`, and `quantity`. It ignores all other columns.

*How can we execute this query efficiently?*

In order to process a query, you may have indexes on `fact_sales.date_key`
and/or `fact_sales.product_sk` that tell the storage engine where to find all
the sales for a particular date or for a particular product. But then, a
row-oriented storage engine still needs to load all of those rows (each
consisting of over 100 attributes) from disk into memory, parse them, and filter
out those that don’t meet the required conditions. That can take a long time.

The idea behind column-oriented storage is simple: don’t store all the values from one
row together, but store all the values from each column together instead. If each col‐
umn is stored in a separate file, a query only needs to read and parse those columns
that are used in that query, which can save a lot of work.

-- ds.image:
src: $assets.files.ddd.data-intensive-application.images.3-3.png

-- ds.markdown:

The column-oriented storage layout relies on each column file containing the rows in
the same order.



-- ds.h2: Column Compression

Take a look at the sequences of values for each column in above image they often
look quite repetitive, which is a good sign for compression.

**Bitmap encoding** is effective compression techniques

-- ds.image:
src: $assets.files.ddd.data-intensive-application.images.3-4.png

-- ds.markdown:

Bitmap indexes such as these are very well suited for the kinds of queries that are
common in a data warehouse. For example:

`WHERE product_sk IN (30, 68, 69):`

Load the three bitmaps for `product_sk = 30`, `product_sk = 68`, and `product_sk
= 69`, and calculate the bitwise *OR* of the three bitmaps, which can be done very
efficiently.

`WHERE product_sk = 31 AND store_sk = 3:`

Load the bitmaps for `product_sk = 31` and `store_sk = 3`, and calculate the
bitwise *AND*. This works because the columns contain the rows in the same order,
so the kth bit in one column’s bitmap corresponds to the same row as the kth bit
in another column’s bitmap.


-- ds.h2: Sort Order in Column Storage

We can choose to impose an order, like we did with SSTables previously, and use
that as an indexing mechanism.

The administrator of the database can choose the columns by which the table
should be sorted, using their knowledge of common queries. For example, if
queries often target date ranges, such as the last month, it might make sense to
make `date_key` the first sort key.

If `date_key` is the first sort key, it might make sense for `product_sk` to be
the second sort key so that all sales for the same product on the same day are
grouped together in storage. That will help queries that need to group or filter
sales by product within a certain date range.

Another advantage of sorted order is that it can help with compression of columns. If
the primary sort column does not have many distinct values, then after sorting, it will
have long sequences where the same value is repeated many times in a row. A simple
run-length encoding, like we used for the bitmaps, could compress that column down
to a few kilobytes—even if the table has billions of rows.

That compression effect is strongest on the first sort key. The second and third sort
keys will be more jumbled up, and thus not have such long runs of repeated values.
Columns further down the sorting priority appear in essentially random order, so
they probably won’t compress as well. But having the first few columns sorted is still
a win overall.

-- ds.h3: Several different sort orders

Having multiple sort orders in a column-oriented store is a bit similar to having
multiple secondary indexes in a row-oriented store. But the big difference is
that the row-oriented store keeps every row in one place (in the heap file or a
clustered index), and secondary indexes just contain pointers to the matching
rows. In a column store, there normally aren’t any pointers to data elsewhere,
only columns containing values.


-- ds.h2: Aggregation: Data Cubes and Materialized Views

Some aggregate function are common, such as COUNT, SUM, AVG, MIN, or MAX in SQL.
If the same aggregates are used by many different queries, it can be wasteful to
crunch through the raw data every time. Why not cache some of the counts or sums
that queries use most often?

**Materialized view** is a table-like object whose contents are the results of
some query.

When the underlying data changes, a materialized view needs to be updated, because
it is a denormalized copy of the data. The database can do that automatically, but
such updates make writes more expensive, which is why materialized views are not
often used in OLTP databases. In read-heavy data warehouses they can make more
sense (whether or not they actually improve read performance depends on the
individual case).

-- ds.image:
src: $assets.files.ddd.data-intensive-application.images.3-5.png


-- end: ds.page
