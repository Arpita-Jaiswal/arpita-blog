-- ds.page: Storage and Retrieval

**Note**: The content is taken from [`Designing Data Intensive Application by
Martin Kleppmann`](https://public.nikhil.io/Designing%20Data%20Intensive%20Applications.pdf)




-- ds.h1: Data Structures That Power Your Database

Consider the world’s simplest database, implemented as two Bash functions:

-- ds.code:
lang: sh

#!/bin/bash
db_set () {
 echo "$1,$2" >> database
}
db_get () {
 grep "^$1," database | sed -e "s/^$1,//" | tail -n 1
}

$ db_set 42 '{"name":"San Francisco","attractions":["Golden Gate Bridge"]}'
$ db_get 42
{"name":"San Francisco","attractions":["Golden Gate Bridge"]}

$ db_set 42 '{"name":"San Francisco","attractions":["Exploratorium"]}'
$ db_get 42
{"name":"San Francisco","attractions":["Exploratorium"]}

$ cat database
42,{"name":"San Francisco","attractions":["Golden Gate Bridge"]}
42,{"name":"San Francisco","attractions":["Exploratorium"]}

-- ds.markdown:

Our db_set function actually has pretty good performance for something that is so
simple, because appending to a file is generally very efficient. Similarly to what
db_set does, many databases internally use a log, which is an append-only data file.

On the other hand, our db_get function has terrible performance if you have a large
number of records in your database. Every time you want to look up a key, db_get
has to scan the entire database file from beginning to end, looking for occurrences of
the key. In algorithmic terms, the cost of a lookup is O(n): if you double the number
of records n in your database, a lookup takes twice as long. That’s not good.

In order to efficiently find the value for a particular key in the database, we need a
different data structure: an *index*.

An index is an additional structure that is derived from the primary data.
Maintaining additional structures incurs overhead, especially on writes. For
writes, it’s hard to beat the performance of simply appending to a file, because
that’s the simplest possible write operation. Any kind of index usually slows down
writes, because the index also needs to be updated every time data is written.

This is an important trade-off in storage systems: well-chosen indexes speed up read
queries, but every index slows down writes. For this reason, databases don’t usually
index everything by default, but require you—the application developer or database
administrator—to choose indexes manually, using your knowledge of the
application’s typical query patterns.





-- ds.h2: Hash Indexes

Let’s say our data storage consists only of appending to a file, as in the preceding
example. Then the simplest possible indexing strategy is this: keep an in-memory
hash map where every key is mapped to a byte offset in the data file—the location at
which the value can be found.

This may sound simplistic, but it is a viable approach. In fact, this is essentially what
Bitcask (the default storage engine in Riak) does. Bitcask offers high-performance
reads and writes, subject to the requirement that all the keys fit in the available RAM,
since the hash map is kept completely in memory.

A storage engine like Bitcask is well suited to situations where the value for each key
is updated frequently. For example, the key might be the URL of a cat video, and the
value might be the number of times it has been played (incremented every time
someone hits the play button). In this kind of workload, there are a lot of writes, but
there are not too many distinct keys—you have a large number of writes per key, but
it’s feasible to keep all keys in memory.

*So how do we avoid eventually running out of disk space?*
A good solution is to break the log into segments of a certain size by closing a
segment file when it reaches a certain size, and making subsequent writes to a
new segment file. We can then perform compaction on these segment. Compaction
means throwing away duplicate keys in the log, and keeping only the most recent
update for each key.

-- ds.code:
lang: txt

\// Data file segment

mew: 1078 | purr: 2108 | purr: 2304 | mew: 409 | yawn: 609 | yawn: 7809 | purr: 908

\// Compaction

yawn: 7809 | purr: 908 | mew: 409

-- ds.markdown:

Since compaction often makes segments much smaller, we can also merge several
segments together at the same time as performing the compaction.

Each segment now has its own in-memory hash table, mapping keys to file offsets. In
order to find the value for a key, we first check the most recent segment’s hash map;
if the key is not present we check the second-most-recent segment, and so on. The
merging process keeps the number of segments small, so lookups don’t need to check
many hash maps.


-- ds.h3: Implementation

Some of the issues that are important in a real implementation are:

- *File format*:

CSV is not the best format for a log. It’s faster and simpler to use a binary format
that first encodes the length of a string in bytes, followed by the raw string

- *Deleting records*:

If you want to delete a key and its associated value, you have to append a special
deletion record to the data file (sometimes called a tombstone). When log
segments are merged, the tombstone tells the merging process to discard any
previous values for the deleted key.

- *Crash recovery*:

Can read the entire segment file from beginning to end and noting the offset of
the most recent value for every key, but then time consuming process.
Bitcask speeds up recovery by storing a snapshot of each segment’s hash map on
disk, which can be loaded into memory more quickly.

- *Partially written records*:

Handle by checking checksum of the record.

- *Concurrency control*:

As writes are appended to the log in a strictly sequential order, a common
implementation choice is to have only one writer thread. Data file segments are
append-only and otherwise immutable, so they can be read concurrently by
multiple threads.


-- ds.h3: Advantage of append-only approach

An append-only log seems wasteful at first glance: why don’t you update the file in
place, overwriting the old value with the new value? But an append-only design turns
out to be good for several reasons:


- Appending and segment merging are sequential write operations, which are
generally much faster than random writes, especially on magnetic spinning-disk
hard drives.

- Concurrency and crash recovery are much simpler if segment files are append
only or immutable. For example, you don’t have to worry about the case where a
crash happened while a value was being overwritten, leaving you with a file
containing part of the old and part of the new value spliced together.


-- ds.h3: Limitation

However, the hash table index also has limitations:

- The hash table must fit in memory, so if you have a very large number of keys,
you’re out of luck.

- Range queries are not efficient. For example, you cannot easily scan over all
keys between kitty00000 and kitty99999—you’d have to look up each key
individually in the hash maps.




-- ds.h2: SSTables and LSM-Trees

The sequence of key-value pairs is sorted by key in our segment files.
We call this format Sorted String Table, or SSTable for short. We also require
that each key only appears once within each merged segment file.

-- ds.h3: Advantage:

- Merging segments is simple and efficient, even if the files are bigger than the
available memory. The approach is like the one used in the mergesort algorithm,
you start reading the input files side by side, look at the first key in each
file, copy the lowest key (according to the sort order) to the output file, and
repeat. This produces a new merged segment file, also sorted by key.

- In order to find a particular key in the file, you no longer need to keep an
index of all the keys in memory. It's sorted so you'll get the key offset from
key's present in the memory. In-memory key can be sparse: one key for every few
kilobytes of segment file is sufficient.

- It is possible to group those records into a block and compress it before
writing it to disk. Each entry of the sparse in-memory index then points at the
start of a compressed block.

-- ds.h3: Constructing and maintaining SSTables

Maintaining a sorted structure on disk is possible but maintaining it in memory
is much easier. There are plenty of well-known tree data structures that you can
use, such as red-black trees or AVL trees. With these data structures, you can
insert keys in any order and read them back in sorted order.

- In-memory balanced tree (eg red-black tree) is called a `memtable`.
- When the memtable gets bigger than some threshold—typically a few megabytes
  write it out to disk as an SSTable file where key-value pairs sorted by key.
  The new SSTable file becomes the most recent segment of the database.
- In order to serve a read request, first try to find the key in the memtable, then in
  the most recent on-disk segment, then in the next-older segment, etc
- Run a merging and compaction process in the background

In case of crash, the memtable can be restored by keeping a separate log on disk.


-- ds.h3: Making an LSM-tree out of SSTables

Storage engines that are based on this principle of merging and compacting sorted
files are often called LSM (Log-Structured Merge-Tree) storage engines.

Lucene, an indexing engine for full-text search used by Elasticsearch and Solr, uses a
similar method for storing its term dictionary [12, 13]. A full-text index is much more
complex than a key-value index but is based on a similar idea: given a word in a
search query, find all the documents (web pages, product descriptions, etc.) that
mention the word. This is implemented with a key-value structure where the key is a
word (a term) and the value is the list of IDs of all the documents that contain the
word (the postings list).


-- ds.h3: Performance optimizations

In order to optimize access, storage engines often use additional *Bloom filters*.
A Bloom filter is a memory-efficient data structure for approximating the contents
of a set. It can tell you if a key does not appear in the database, and thus
saves many unnecessary disk reads for nonexistent keys



-- ds.h2: B-Trees

B-trees break the database down into fixed-size blocks or pages, traditionally
4 KB in size (sometimes bigger), and read or write one page at a time. This
design corresponds more closely to the underlying hardware, as disks are also arranged
in fixed-size blocks.

Each page can be identified using an address or location, which allows one page to
refer to another on disk. We can use these page references to construct a tree of pages

-- ds.code:
lang: txt

Look up for user_id 251

[100, **200**, 300, 400, ...]
         |
[200, 210, 220, 230, 240, **250**,...]
                             |
[250, **251**, 252, 253, 254, ...]

-- ds.markdown:

This algorithm ensures that the tree remains balanced: a B-tree with n keys always
has a depth of O(log n). Most databases can fit into a B-tree that is three or four levels
deep, so you don’t need to follow many page references to find the page you are
looking for. (A four-level tree of 4 KB pages with a branching factor of 500 can
store up to 256 TB.)

In order to make the database resilient to crashes, it is common for B-tree
implementations to include an additional data structure on disk: a write-ahead
log (WAL, also known as a redo log). This is an append-only file to which every
B-tree modification must be written before it can be applied to the pages of the
tree itself.


-- end: ds.page
