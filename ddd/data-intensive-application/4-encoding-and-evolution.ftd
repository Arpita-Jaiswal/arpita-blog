-- ds.page: Encoding and Evolution

**Note**: The content is taken from [`Designing Data Intensive Application by
Martin Kleppmann`](https://public.nikhil.io/Designing%20Data%20Intensive%20Applications.pdf)

**Backward compatibility**: Newer code can read data that was written by older code.

**Forward compatibility**: Older code can read data that was written by newer code.

-- ds.h1: Formats for Encoding Data

Programs usually work with data in (at least) two different representations:

- In memory, data is kept in objects, structs, lists, arrays, hash tables, trees, and so
  on.
- When you want to write data to a file or send it over the network, you have to
  encode it as some kind of self-contained sequence of bytes (for example, a JSON
  document). A pointer wouldn’t make sense to any other process.

Thus, we need some kind of translation between the two representations. The
translation from the in-memory representation to a byte sequence is called *encoding*
(also known as serialization or marshalling), and the reverse is called *decoding* (parsing,
deserialization, unmarshalling)

-- ds.h2: Language-Specific Formats

They have a number of deep problems:

- The encoding is often tied to a particular programming language, and reading
  the data in another language is very difficult.
- In order to restore data in the same object types, the decoding process needs to
  be able to instantiate arbitrary classes. This is frequently a source of security
  problems: if an attacker can get your application to decode an arbitrary byte
  sequence, they can instantiate arbitrary classes, which in turn often allows them
  to do terrible things such as remotely executing arbitrary code.
- Often neglect the inconvenient problems of forward and backward compatibility.
- Efficiency (CPU time taken to encode or decode, and the size of the encoded
  structure) is also often an afterthought.

-- ds.h2: JSON, XML, and Binary Variants

Moving to standardized encodings that can be written and read by many program‐
ming languages, JSON and XML are the obvious contenders.

They also have some subtle problems:

- There is a lot of ambiguity around the encoding of numbers. In XML and CSV,
  you cannot distinguish between a number and a string that happens to consist of
  digits. JSON distinguishes strings and numbers, but it doesn’t distinguish
  integers and floating-point numbers, and it doesn’t specify a precision.
- JSON and XML have good support for Unicode character strings, but they don’t
  support binary strings
- CSV does not have any schema, so it is up to the application to define the
  meaning of each row and column. If an application change adds a new row or
  column, you have to handle that change manually.

Despite these flaws, JSON, XML, and CSV are good enough for many purposes.

-- ds.h2: Binary encoding

JSON is less verbose than XML, but both still use a lot of space compared to binary
formats.

MessagePack is a binary encoding for JSON. For following JSON,

-- ds.code:
lang: json

{
 "userName": "Martin",
 "favoriteNumber": 1337,
 "interests": ["daydreaming", "hacking"]
}

-- ds.markdown:

- The first byte, 0x83, indicates that what follows is an object (top four bits = 0x80)
  with three fields (bottom four bits = 0x03).
- The second byte, 0xa8, indicates that what follows is a string (top four bits =
  0xa0) that is eight bytes long (bottom four bits = 0x08).
- The next eight bytes are the field name userName in ASCII.
- The next seven bytes encode the six-letter string value Martin with a prefix 0xa6,
  and so on.

The binary encoding is 66 bytes long, which is only a little less than the 81 bytes taken
by the textual JSON encoding (with whitespace removed).

-- ds.image:
src: $assets.files.ddd.data-intensive-application.images.4-1.png

-- ds.h2: Thrift and Protocol Buffers

Apache Thrift and Protocol Buffers (protobuf) are binary encoding libraries that
are based on the same principle. Protocol Buffers was originally developed at
Google, Thrift was originally developed at Facebook, and both were made open
source in 2007–08.

Thrift has two different binary encoding formats, called **BinaryProtocol** and
**CompactProtocol**, respectively. Encoding above JSON in BinaryProtocol format
takes 59 bytes.

The Thrift CompactProtocol packs the same information into only 34 bytes.

Protocol Buffers fits the same record in 33 bytes.

For more details, check the pdf link shared at the start of this chapter.

-- ds.h2: Avro

Apache Avro is another binary encoding format that is interestingly different
from Protocol Buffers and Thrift. It was started in 2009 as a subproject of
Hadoop, as a result of Thrift not being a good fit for Hadoop’s use cases.

Avro also uses a schema to specify the structure of the data being encoded. It has two
schema languages: one (Avro IDL) intended for human editing, and one (based on
JSON) that is more easily machine-readable.

-- ds.code: Our example schema, written in Avro IDL, might look like this:
lang: rs

record Person {
 string userName;
 union { null, long } favoriteNumber = null;
 array<string> interests;
}

-- ds.code: The equivalent JSON representation of that schema is as follows:
lang: json

{
 "type": "record",
 "name": "Person",
 "fields": [
 {"name": "userName", "type": "string"},
 {"name": "favoriteNumber", "type": ["null", "long"], "default": null},
 {"name": "interests", "type": {"type": "array", "items": "string"}}
 ]
}

-- ds.markdown:

First of all, notice that there are no tag numbers in the schema. The Avro binary
encoding is just 32 bytes long—the most compact of all the encodings we have seen.

-- ds.image:
src: $assets.files.ddd.data-intensive-application.images.4-2.png

-- ds.markdown:

To parse the binary data, you go through the fields in the order that they appear in
the schema and use the schema to tell you the datatype of each field. This means that
the binary data can only be decoded correctly if the code reading the data is using the
exact same schema as the code that wrote the data.

So, how does Avro support schema evolution?

-- ds.h3: The writer’s schema and the reader’s schema

With Avro, when an application wants to encode some data (to write it to a file or
database, to send it over the network, etc.), it encodes the data using whatever version
of the schema it knows about—for example, that schema may be compiled into the
application. This is known as the **writer’s schema**.

When an application wants to decode some data (read it from a file or database,
receive it from the network, etc.), it is expecting the data to be in some schema, which
is known as the **reader’s schema**. That is the schema the application code is relying on
—code may have been generated from that schema during the application’s build
process.

The key idea with Avro is that the writer’s schema and the reader’s schema don’t have
to be the same—they only need to be compatible. When data is decoded (read), the
Avro library resolves the differences by looking at the writer’s schema and the
reader’s schema side by side and translating the data from the writer’s schema into
the reader’s schema.

For example, it’s no problem if the writer’s schema and the reader’s schema have
their fields in a different order, because the schema resolution matches up the fields
by field name. If the code reading the data encounters a field that appears in the
writer’s schema but not in the reader’s schema, it is ignored. If the code reading the
data expects some field, but the writer’s schema does not contain a field of that name,
it is filled in with a default value declared in the reader’s schema.

-- ds.image:
src: $assets.files.ddd.data-intensive-application.images.4-3.png

-- ds.h3: Schema evolution rules

With Avro, **forward compatibility** means that you can have a new version of the
schema as writer and an old version of the schema as reader. Conversely, **backward
compatibility** means that you can have a new version of the schema as reader and an
old version as writer.

To maintain compatibility, you may only add or remove a field that has a default
value. (The field favoriteNumber in our Avro schema has a default value of null.)
For example, say you add a field with a default value, so this new field exists in the
new schema but not the old one. When a reader using the new schema reads a record
written with the old schema, the default value is filled in for the missing field.

If you were to add a field that has no default value, new readers wouldn’t be able to
read data written by old writers, so you would break backward compatibility. If you
were to remove a field that has no default value, old readers wouldn’t be able to read
data written by new writers, so you would break forward compatibility








-- ds.h1: Modes of Dataflow

Some of the most common ways how data flows between processes:

• Via databases (“Dataflow Through Databases”)
• Via service calls (“Dataflow Through Services: REST and RPC”)
• Via asynchronous message passing (“Message-Passing Dataflow”)


-- ds.h2: Dataflow Through Databases

In a database, the process that writes to the database encodes the data, and the
process that reads from the database decodes it. There may just be a single process
accessing the database, in which case the reader is simply a later version of the same
process—in that case you can think of storing something in the database as sending a
message to your future self.

Backward compatibility is clearly necessary here; otherwise your future self won’t be
able to decode what you previously wrote.

In an environment where the application is changing, it is likely that some
processes accessing the database will be running newer code and some will be
running older code.

This means that a value in the database may be written by a **newer** version of the
code, and subsequently read by an **older** version of the code that is still running.
Thus, forward compatibility is also often required for databases.

If you decode a database value into model objects in the application, and later
re-encode those model objects, the unknown field might be lost in that translation
process. Solving this is not a hard problem; you just need to be aware of it.

-- ds.image:
src: $assets.files.ddd.data-intensive-application.images.4-4.png


-- ds.h2: Dataflow Through Services: REST and RPC

The **servers** expose an API over the network, and the **clients** can connect to
the servers to make requests to that API. The API exposed by the server is known
as a **service**.

The web works this way: clients (web browsers) make requests to web servers,
making GET requests to download HTML, CSS, JavaScript, images, etc., and making POST
requests to submit data to the server. The API consists of a standardized set of
protocols and data formats (HTTP, URLs, SSL/TLS, HTML, etc.).

Web browsers are not the only type of client. For example, a native app running on a
mobile device or a desktop computer can also make network requests to a server, and
a client-side JavaScript application running inside a web browser can use
XMLHttpRequest to become an HTTP client (this technique is known as Ajax).

In this case, the server’s response is typically not HTML for displaying to a human,
but rather data in an encoding that is convenient for further processing by the
clientside application code (such as JSON). Although HTTP may be used as the transport
protocol, the API implemented on top is application-specific, and the client and
server need to agree on the details of that API.

Moreover, a server can itself be a client to another service (for example, a typical web
app server acts as client to a database). This approach is often used to decompose a
large application into smaller services by area of functionality, such that one service
makes a request to another when it requires some functionality or data from that
other service. This way of building applications has traditionally been called a
**service oriented architecture** (SOA), more recently refined and rebranded as **microservices
architecture**.

We should expect old and new versions of servers and clients to be running at the
same time, and so the data encoding used by servers and clients must be compatible
across versions of the service API.


-- ds.h3: Web services

When HTTP is used as the underlying protocol for talking to the service, it is called a
web service. This is perhaps a slight misnomer, because web services are not only used
on the web, but in several different contexts. For example:

- A client application running on a user’s device (e.g., a native app on a mobile
  device, or JavaScript web app using Ajax) making requests to a service over
  HTTP. These requests typically go over the public internet.

- One service making requests to another service owned by the same organization,
  often located within the same datacenter, as part of a service-oriented/microser‐
  vices architecture. (Software that supports this kind of use case is sometimes
  called middleware.)

- One service making requests to a service owned by a different organization, usu‐
  ally via the internet. This is used for data exchange between different organiza‐
  tions’ backend systems. This category includes public APIs provided by online
  services, such as credit card processing systems, or OAuth for shared access to
  user data.

There are two popular approaches to web services: REST and SOAP.

REST is not a protocol, but rather a design philosophy that builds upon the principles
of HTTP. It emphasizes simple data formats, using URLs for identifying resources
and using HTTP features for cache control, authentication, and content type negotiation.

By contrast, SOAP is an XML-based protocol for making network API requests.vii
Although it is most commonly used over HTTP, it aims to be independent from
HTTP and avoids using most HTTP features. Instead, it comes with a sprawling and
complex multitude of related standards (the web service framework, known as WS-*)
that add various features.

The API of a SOAP web service is described using an XML-based language called the
Web Services Description Language, or WSDL. WSDL enables code generation so
that a client can access a remote service using local classes and method calls.
This is useful in statically typed programming languages, but less so in dynamically
typed ones.

As WSDL is not designed to be human-readable, and as SOAP messages are often too
complex to construct manually, users of SOAP rely heavily on tool support, code
generation, and IDEs.

RESTful APIs tend to favor simpler approaches, typically involving less code genera‐
tion and automated tooling. A definition format such as OpenAPI, also known as
**Swagger**, can be used to describe RESTful APIs and produce documentation.


-- ds.h3: The problems with remote procedure calls (RPCs)

The **remote procedure call** (RPC) model tries to make a request to a remote
network service look the same as calling a function or method in your programming
language, within the same process (this abstraction is called **location transparency**).

The approach is fundamentally flawed. A network request is very different from a
local function call:

- A local function call is predictable and either succeeds or fails, depending only on
  parameters that are under your control. A network request is unpredictable: the
  request or response may be lost due to a network problem, or the remote
  machine may be slow or unavailable, and such problems are entirely outside of
  your control.

- A local function call either returns a result, or throws an exception, or never
  returns. A network request has another possible outcome: it may return without
  a result, due to a timeout. In that case, you simply don’t know what happened.

- If you retry a failed network request, it could happen that the requests are
  actually getting through, and only the responses are getting lost. In that case,
  retrying will cause the action to be performed multiple times.

- Every time you call a local function, it normally takes about the same time to exe‐
  cute. A network request is much slower than a function call, and its latency is
  also wildly variable.

- When you call a local function, you can efficiently pass it references (pointers) to
  objects in local memory. When you make a network request, all those parameters
  need to be encoded into a sequence of bytes that can be sent over the network.

- The client and the service may be implemented in different programming
  languages, so the RPC framework must translate datatypes from one language into
  another.

-- ds.h3: Current directions for RPC

This new generation of RPC frameworks is more explicit about the fact that a remote
request is different from a local function call. For example, Finagle and Rest.li use
**futures** (promises) to encapsulate asynchronous actions that may fail. Futures also
simplify situations where you need to make requests to multiple services in parallel,
and combine their results. gRPC supports **streams**, where a call consists of not
just one request and one response, but a series of requests and responses over time.

Some of these frameworks also provide **service discovery** —that is, allowing a client to
find out at which IP address and port number it can find a particular service.

Custom RPC protocols with a binary encoding format can achieve better performance
than something generic like JSON over REST. However, a RESTful API has other
significant advantages: it is good for experimentation and debugging (you can
simply make requests to it using a web browser or the command-line tool curl,
without any code generation or software installation), it is supported by all
mainstream programming languages and platforms, and there is a vast ecosystem of tools
available (servers, caches, load balancers, proxies, firewalls, monitoring, debugging
tools, testing tools, etc.).


-- ds.h3: Data encoding and evolution for RPC

It is reasonable to assume that all the servers will be updated first, and all
the clients second. Thus, you only need backward compatibility on requests, and
forward compatibility on responses.

There is no agreement on how API versioning should work (i.e., how a client can
indicate which version of the API it wants to use [48]). For RESTful APIs, common
approaches are to use a version number in the URL or in the HTTP Accept header.


-- ds.h2: Message-Passing Dataflow

**Asynchronous message-passing systems** are somewhere between RPC and databases.
They are similar to RPC in that a client’s request (usually called a **message**)
is delivered to another process with low latency. They are similar to databases in
that the message is not sent via a direct network connection, but goes via an
intermediary called a **message broker** (also called a message queue or
message-oriented middleware), which stores the message temporarily.

Using a message broker has several advantages compared to direct RPC:

- It can act as a buffer if the recipient is unavailable or overloaded, and thus
improve system reliability.
- It can automatically redeliver messages to a process that has crashed, and thus
prevent messages from being lost.
- It avoids the sender needing to know the IP address and port number of the
recipient (which is particularly useful in a cloud deployment where virtual
machines often come and go).
- It allows one message to be sent to several recipients.
- It logically decouples the sender from the recipient (the sender just publishes
messages and doesn’t care who consumes them).

However, a difference compared to RPC is that message-passing communication is
usually one-way: a sender normally doesn’t expect to receive a reply to its messages.
This communication pattern is **asynchronous**.

-- ds.h3: Message brokers

In the past, the landscape of message brokers was dominated by commercial enterprise
software from companies such as TIBCO, IBM WebSphere, and webMethods. More recently,
open source implementations such as RabbitMQ, ActiveMQ, HornetQ, NATS, and Apache
Kafka have become popular.

Message brokers are used as follows: one process sends a message to a named
**queue** or **topic**, and the broker ensures that the message is delivered to
one or more **consumers** of or **subscribers** to that queue or topic. There can
be many producers and many consumers on the same topic.

A topic provides only one-way dataflow. However, a consumer may itself publish
messages to another topic, or to a reply queue that is consumed by the sender of
the original message (allowing a request/response dataflow, similar to RPC).

-- ds.h3: Distributed actor frameworks

The **actor model** is a programming model for concurrency in a single process. Rather
than dealing directly with threads (and the associated problems of race conditions,
locking, and deadlock), logic is encapsulated in actors. Each actor typically represents
one client or entity, it may have some local state (which is not shared with any other
actor), and it communicates with other actors by sending and receiving asynchronous
messages. Message delivery is not guaranteed: in certain error scenarios, messages
will be lost. Since each actor processes only one message at a time, it doesn’t
need to worry about threads, and each actor can be scheduled independently by the
framework.

In **distributed actor frameworks**, this programming model is used to scale an
application across multiple nodes. The same message-passing mechanism is used, no matter
whether the sender and recipient are on the same node or different nodes. If they are
on different nodes, the message is transparently encoded into a byte sequence, sent
over the network, and decoded on the other side.

Location transparency works better in the actor model than in RPC, because the actor
model already assumes that messages may be lost, even within a single process.
Although latency over the network is likely higher than within the same process,
there is less of a fundamental mismatch between local and remote communication
when using the actor model.

Three popular distributed actor frameworks handle message encoding as follows:

- Akka uses Java’s built-in serialization by default, which does not provide forward
  or backward compatibility. However, you can replace it with something like Protocol
  Buffers, and thus gain the ability to do rolling upgrades.
- Orleans by default uses a custom data encoding format that does not support
  rolling upgrade deployments; to deploy a new version of your application, you
  need to set up a new cluster, move traffic from the old cluster to the new one, and
  shut down the old one. Like with Akka, custom serialization plug-ins can be used.
- In Erlang OTP it is surprisingly hard to make changes to record schemas (despite
  the system having many features designed for high availability); rolling upgrades
  are possible but need to be planned carefully. An experimental new maps datatype
  (a JSON-like structure, introduced in Erlang R17 in 2014) may make this easier in the future.



-- end: ds.page
