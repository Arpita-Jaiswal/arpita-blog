-- ds.page: Replication

**Note**: The content is taken from [`Designing Data Intensive Application by
Martin Kleppmann`](https://public.nikhil.io/Designing%20Data%20Intensive%20Applications.pdf)

In this chapter we will assume that your dataset is so small that each machine can
hold a copy of the entire dataset. In later chapter we will relax that assumption and
discuss partitioning (sharding) of datasets that are too big for a single machine.

-- ds.h1: Leaders and Followers

- One of the replicas is designated the leader (also known as master or primary).
  When clients want to write to the database, they must send their requests to the
  leader, which first writes the new data to its local storage.
- The other replicas are known as followers (read replicas, slaves, secondaries, or hot
  standbys). Whenever the leader writes new data to its local storage, it also sends
  the data change to all of its followers as part of a replication log or change stream.
- When a client wants to read from the database, it can query either the leader or
  any of the followers. However, writes are only accepted on the leader (the follow‐
  ers are read-only from the client’s point of view).

-- ds.image:
src: $assets.files.ddd.data-intensive-application.images.5-1.png

-- ds.markdown:

This mode of replication is a built-in feature of many relational databases,
such as PostgreSQL (since version 9.0), MySQL, Oracle Data Guard, and SQL Server’s
AlwaysOn Availability Groups. It is also used in some nonrelational databases,
including MongoDB, RethinkDB, and Espresso. Finally, leader-based replication is
not restricted to only databases: distributed message brokers such as Kafka and
RabbitMQ highly available queues also use it. Some network filesystems and
replicated block devices such as DRBD are similar.

-- ds.h2: Synchronous Versus Asynchronous Replication

-- ds.image:
src: $assets.files.ddd.data-intensive-application.images.5-2.png

-- ds.markdown:

The replication to follower 1 is synchronous: the leader waits until follower 1
has confirmed that it received the write before reporting success to the user.
The replication to follower 2 is asynchronous: the leader sends the message, but
doesn’t wait for a response from the follower.

The **advantage** of synchronous replication is that the follower is guaranteed
to have an up-to-date copy of the data that is consistent with the leader. If
the leader suddenly fails, we can be sure that the data is still available on
the follower. The **disadvantage** is that if the synchronous follower doesn’t
respond (because it has crashed, or there is a network fault, or for any other
reason), the write cannot be processed. The leader must block all writes and wait
until the synchronous replica is available again.

For that reason, it is impractical for all followers to be synchronous. In
practice, if you enable synchronous replication on a database, it usually means
that one of the followers is synchronous, and the others are asynchronous. If
the synchronous follower becomes unavailable or slow, one of the asynchronous
followers is made synchronous. This guarantees that you have an up-to-date copy
of the data on at least two nodes. This configuration is sometimes also called
semi-synchronous.

Often, leader-based replication is configured to be completely asynchronous. In
this case, if the leader fails and is not recoverable. This means that a write
is not  guaranteed to be durable, even if it has been confirmed to the client.

Weakening durability may sound like a bad trade-off, but asynchronous
replication is nevertheless widely used, especially if there are many followers
or if they are geo‐graphically distributed.

-- ds.h2: Setting Up New Followers

Simply copying data files from one node to another is typically not sufficient:
clients are constantly writing to the database, and the data is always in flux,
so a standard file copy would see different parts of the database at different
points in time.

You could make the files on disk consistent by locking the database (making it
unavailable for writes), but that would go against our goal of high availability.
Fortunately, setting up a follower can usually be done without downtime.
Conceptually, the process looks like this:

- Take a consistent snapshot of the leader’s database at some point in time—if
  possible, without taking a lock on the entire database.
- Copy the snapshot to the new follower node.
- The follower connects to the leader and requests all the data changes that
  have happened since the snapshot was taken. This requires that the snapshot
  is associated with an exact position in the leader’s replication log. That
  position has various names: for example, PostgreSQL calls it the *log sequence
  number*, and MySQL calls it the *binlog coordinates*.
- When the follower has processed the backlog of data changes since the
  snapshot, we say it has *caught up*.

The practical steps of setting up a follower vary significantly by database. In
some systems the process is fully automated, whereas in others it can be a
somewhat arcane multi-step workflow that needs to be manually performed by an
administrator.


-- ds.h2: Handling Node Outages

Any node in the system can go down, perhaps unexpectedly due to a fault, but
just as likely due to planned maintenance (for example, rebooting a machine to
install a kernel security patch).

How do you achieve high availability with leader-based replication?

-- ds.h3: Follower failure: Catch-up recovery

On its local disk, each follower keeps a log of the data changes it has received
from the leader. If a follower crashes and is restarted, or if the network
between the leader and the follower is temporarily interrupted, the follower
can recover quite easily: from its log, it knows the last transaction that was
processed before the fault occurred. Thus, the follower can connect to the
leader and request all the data changes that occurred during the time when the
follower was disconnected. When it has applied these changes, it has caught up
to the leader and can continue receiving a stream of data changes as before.


-- ds.h3: Leader failure: Failover

Handling a failure of the leader is trickier: one of the followers needs to be
promoted to be the new leader, clients need to be reconfigured to send their
writes to the new leader, and the other followers need to start consuming data
changes from the new leader. This process is called **failover**.

- **Determining that the leader has failed**. There are many things that could
  potentially go wrong: crashes, power outages, network issues, and more.
  Typically done through timeouts as there's no foolproof method. If a node
  doesn't respond within a set time, it's assumed dead.
- **Choosing a new leader**. This could be done through an election process, or
  a new leader could be appointed by a previously elected controller node. The
  best candidate for leadership is usually the replica with the most up-to-date
  data changes from the old leader.
- **Reconfiguring the system to use the new leader**. Clients now need to send
  their write requests to the new leader. If the old leader comes back, it might
  still believe that it is the leader. The system needs to ensure that the old
  leader becomes a follower and recognizes the new leader.

Failover is fraught with things that can go wrong:

- If asynchronous replication is used, the new leader may not have received all
  the writes, If the former leader rejoins the cluster after a new leader has
  been chosen, what should happen to those writes? The new leader may have
  received conflicting writes in the meantime. The most common solution is for
  the old leader’s unreplicated writes to simply be discarded, which may violate
  clients’ durability expectations.
- Discarding writes is especially dangerous if other storage systems outside of
  the database need to be coordinated with the database contents. For example,
  in one incident at GitHub, an out-of-date MySQL follower was promoted to
  leader. The database used an autoincrementing counter to assign primary keys
  to new rows, but because the new leader’s counter lagged behind the old
  leader’s, it reused some primary keys that were previously assigned by the old
  leader. These primary keys were also used in a Redis store,so the reuse of
  primary keys resul‐ ted in inconsistency between MySQL and Redis, which caused
  some private data to be disclosed to the wrong users.
- In certain fault scenarios, it could happen that two nodes both believe that
  they are the leader. This situation is called split brain, and it is
  dangerous: if both leaders accept writes, and there is no process for
  resolving conflicts, data is likely to be lost or corrupted. As a safety
  catch, some systems have a mechanism to shut down one node if two leaders are
  detected.
- What is the right timeout before the leader is declared dead? A longer timeout
  means a longer time to recovery in the case where the leader fails. However,
  if the timeout is too short, there could be unnecessary failovers. For
  example, a temporary load spike could cause a node’s response time to increase
  above the timeout, or a network glitch could cause delayed packets. If the
  system is already struggling with high load or network problems, an
  unnecessary failover is likely to make the situation worse, not better.




-- ds.h2: Implementation of Replication Logs

-- ds.h3: Statement-based replication

In the simplest case, the leader logs every write request (statement) that it
executes and sends that statement log to its followers. For a relational
database, this means that every INSERT, UPDATE, or DELETE statement is forwarded
to followers, and each follower parses and executes that SQL statement as if it
had been received from a client. Although this may sound reasonable, there are
various ways in which this approach to replication can break down:

- Nondeterministic functions like NOW() or RAND() may produce different results
  on replicas.
- Operations dependent on autoincrementing columns or existing data (e.g.,
  UPDATE ... WHERE <some condition>) must execute in the same order on all
  replicas to maintain consistency, which can be challenging with concurrent
  transactions.
- Statements with side effects (e.g., triggers, stored procedures) may produce
  different different side effects occurring on each replica unless they are
  entirely deterministic.

It is possible to work around those issues—for example, the leader can replace
any nondeterministic function calls with a fixed return value when the statement
is logged so that the followers all get the same value. However, because there
are so many edge cases, other replication methods are now generally preferred.

Statement-based replication was used in MySQL before version 5.1. It is still
sometimes used today, as it is quite compact, but by default MySQL now switches
to row-based replication (discussed shortly) if there is any nondeterminism in a
statement. VoltDB uses statement-based replication, and makes it safe by
requiring transactions to be deterministic.

-- ds.h3: Write-ahead log (WAL) shipping

- In the case of a log-structured storage engine (“SSTables and LSM-Trees”),
  this log is the main place for storage. Log segments are compacted and
  garbage-collected in the background.
- In the case of a "B-tree", which overwrites individual disk blocks, every
  modification is first written to a write-ahead log so that the index can be
  restored to a consistent state after a crash.

In either case, the log is an append-only sequence of bytes containing all
writes to the database. We can use the exact same log to build a replica on
another node.

This method of replication is used in PostgreSQL and Oracle, among others.
Despite its advantages, WAL shipping has drawbacks. It tightly couples
replication with the storage engine, making it challenging to run different
database software versions on the leader and followers. This limitation can
impact operational tasks like zero-downtime upgrades, which are feasible only if
the replication protocol supports version mismatches between leader and
followers. However, WAL shipping often requires downtime for such upgrades due
to its strict version matching requirements.

-- ds.h3: Logical (row-based) log replication

An alternative approach involves using different log formats for replication and
storage engines, allowing the replication log to be decoupled from storage
engine internals. Known as a logical log, this format differs from the physical
data representation of the storage engine.

In a relational database, a logical log typically consists of records describing
writes to database tables at the row level:

- For inserted rows, the log contains the new values of all columns.
- For deleted rows, it includes adequate information to uniquely identify the
  deleted row, often the primary key or, if unavailable, the old values of all
  columns.
- For updated rows, it includes information to uniquely identify the row and the
  new values of changed columns.

Transactions modifying multiple rows generate corresponding log records,
followed by a commit record. MySQL's binlog, configured for row-based
replication, adopts this approach.

A logical log format is also easier for external applications to parse. This
aspect is useful if you want to send the contents of a database to an external
system, such as a data warehouse for offline analysis, or for building custom
indexes and caches. This technique is called **change data capture**.

-- ds.h3: Trigger-based replication

The replication approaches described so far are implemented by the database
system, without involving any application code. You may need to move replication
up to the application layer. For example, if you want to only replicate a subset
of the data, or want to replicate from one kind of database to another, or if
you need conflict resolution logic.

Some tools, such as Oracle GoldenGate, can make data changes available to an
application by reading the database log. An alternative is to use features that
are available in many relational databases: **triggers** and **stored
procedures**.

A **trigger** lets you register custom application code that is automatically
executed when a data change (write transaction) occurs in a database system. The
trigger has the opportunity to log this change into a separate table, from which
it can be read by an external process. That external process can then apply any
necessary application logic and replicate the data change to another system.
Databus for Oracle and Bucardo for Postgres work like this, for example.

Trigger-based replication typically has greater overheads than other replication
methods, and is more prone to bugs and limitations than the database’s built-in
replication. However, it can nevertheless be useful due to its flexibility.


-- ds.h1: Problems with Replication Lag

If an application reads from an asynchronous follower, it may see out‐ dated
information if the follower has fallen behind. This inconsistency is just a
temporary state— for that reason, this effect is known as eventual consistency.

In this section we will highlight three examples of problems that are likely to
occur when there is replication lag and outline some approaches to solving them.

-- ds.h2: Reading Your Own Writes

With asynchronous replication, there is a problem, if the user views the data
shortly after making a write, the new data may not yet have reached the replica.

-- ds.image:
src: $assets.files.ddd.data-intensive-application.images.5-3.png

-- end: ds.page
