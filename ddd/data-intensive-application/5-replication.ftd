-- ds.page: Replication

**Note**: The content is taken from [`Designing Data Intensive Application by
Martin Kleppmann`](https://public.nikhil.io/Designing%20Data%20Intensive%20Applications.pdf)

In this chapter we will assume that your dataset is so small that each machine can
hold a copy of the entire dataset. In later chapter we will relax that assumption and
discuss partitioning (sharding) of datasets that are too big for a single machine.

-- ds.h1: Leaders and Followers

- One of the replicas is designated the leader (also known as master or primary).
  When clients want to write to the database, they must send their requests to the
  leader, which first writes the new data to its local storage.
- The other replicas are known as followers (read replicas, slaves, secondaries, or hot
  standbys). Whenever the leader writes new data to its local storage, it also sends
  the data change to all of its followers as part of a replication log or change stream.
- When a client wants to read from the database, it can query either the leader or
  any of the followers. However, writes are only accepted on the leader (the follow‐
  ers are read-only from the client’s point of view).

-- ds.image:
src: $assets.files.ddd.data-intensive-application.images.5-1.png

-- ds.markdown:

This mode of replication is a built-in feature of many relational databases,
such as PostgreSQL (since version 9.0), MySQL, Oracle Data Guard, and SQL Server’s
AlwaysOn Availability Groups. It is also used in some nonrelational databases,
including MongoDB, RethinkDB, and Espresso. Finally, leader-based replication is
not restricted to only databases: distributed message brokers such as Kafka and
RabbitMQ highly available queues also use it. Some network filesystems and
replicated block devices such as DRBD are similar.

-- ds.h2: Synchronous Versus Asynchronous Replication

-- ds.image:
src: $assets.files.ddd.data-intensive-application.images.5-2.png

-- ds.markdown:

The replication to follower 1 is synchronous: the leader waits until follower 1
has confirmed that it received the write before reporting success to the user.
The replication to follower 2 is asynchronous: the leader sends the message, but
doesn’t wait for a response from the follower.

The **advantage** of synchronous replication is that the follower is guaranteed
to have an up-to-date copy of the data that is consistent with the leader. If
the leader suddenly fails, we can be sure that the data is still available on
the follower. The **disadvantage** is that if the synchronous follower doesn’t
respond (because it has crashed, or there is a network fault, or for any other
reason), the write cannot be processed. The leader must block all writes and wait
until the synchronous replica is available again.

For that reason, it is impractical for all followers to be synchronous. In
practice, if you enable synchronous replication on a database, it usually means
that one of the followers is synchronous, and the others are asynchronous. If
the synchronous follower becomes unavailable or slow, one of the asynchronous
followers is made synchronous. This guarantees that you have an up-to-date copy
of the data on at least two nodes. This configuration is sometimes also called
semi-synchronous.

Often, leader-based replication is configured to be completely asynchronous. In
this case, if the leader fails and is not recoverable. This means that a write
is not  guaranteed to be durable, even if it has been confirmed to the client.

Weakening durability may sound like a bad trade-off, but asynchronous
replication is nevertheless widely used, especially if there are many followers
or if they are geo‐graphically distributed.

-- ds.h2: Setting Up New Followers

Simply copying data files from one node to another is typically not sufficient:
clients are constantly writing to the database, and the data is always in flux,
so a standard file copy would see different parts of the database at different
points in time.

You could make the files on disk consistent by locking the database (making it
unavailable for writes), but that would go against our goal of high availability.
Fortunately, setting up a follower can usually be done without downtime.
Conceptually, the process looks like this:

- Take a consistent snapshot of the leader’s database at some point in time—if
  possible, without taking a lock on the entire database.
- Copy the snapshot to the new follower node.
- The follower connects to the leader and requests all the data changes that
  have happened since the snapshot was taken. This requires that the snapshot
  is associated with an exact position in the leader’s replication log. That
  position has various names: for example, PostgreSQL calls it the *log sequence
  number*, and MySQL calls it the *binlog coordinates*.
- When the follower has processed the backlog of data changes since the
  snapshot, we say it has *caught up*.

The practical steps of setting up a follower vary significantly by database. In
some systems the process is fully automated, whereas in others it can be a
somewhat arcane multi-step workflow that needs to be manually performed by an
administrator.


-- ds.h2: Handling Node Outages

Any node in the system can go down, perhaps unexpectedly due to a fault, but
just as likely due to planned maintenance (for example, rebooting a machine to
install a kernel security patch).

How do you achieve high availability with leader-based replication?

-- ds.h3: Follower failure: Catch-up recovery

On its local disk, each follower keeps a log of the data changes it has received
from the leader. If a follower crashes and is restarted, or if the network
between the leader and the follower is temporarily interrupted, the follower
can recover quite easily: from its log, it knows the last transaction that was
processed before the fault occurred. Thus, the follower can connect to the
leader and request all the data changes that occurred during the time when the
follower was disconnected. When it has applied these changes, it has caught up
to the leader and can continue receiving a stream of data changes as before.


-- ds.h3: Leader failure: Failover

Handling a failure of the leader is trickier: one of the followers needs to be
promoted to be the new leader, clients need to be reconfigured to send their
writes to the new leader, and the other followers need to start consuming data
changes from the new leader. This process is called **failover**.

- **Determining that the leader has failed**. There are many things that could
  potentially go wrong: crashes, power outages, network issues, and more.
  Typically done through timeouts as there's no foolproof method. If a node
  doesn't respond within a set time, it's assumed dead.
- **Choosing a new leader**. This could be done through an election process, or
  a new leader could be appointed by a previously elected controller node. The
  best candidate for leadership is usually the replica with the most up-to-date
  data changes from the old leader.
- **Reconfiguring the system to use the new leader**. Clients now need to send
  their write requests to the new leader. If the old leader comes back, it might
  still believe that it is the leader. The system needs to ensure that the old
  leader becomes a follower and recognizes the new leader.

Failover is fraught with things that can go wrong:

- If asynchronous replication is used, the new leader may not have received all
  the writes, If the former leader rejoins the cluster after a new leader has
  been chosen, what should happen to those writes? The new leader may have
  received conflicting writes in the meantime. The most common solution is for
  the old leader’s unreplicated writes to simply be discarded, which may violate
  clients’ durability expectations.
- Discarding writes is especially dangerous if other storage systems outside of
  the database need to be coordinated with the database contents. For example,
  in one incident at GitHub, an out-of-date MySQL follower was promoted to
  leader. The database used an autoincrementing counter to assign primary keys
  to new rows, but because the new leader’s counter lagged behind the old
  leader’s, it reused some primary keys that were previously assigned by the old
  leader. These primary keys were also used in a Redis store,so the reuse of
  primary keys resul‐ ted in inconsistency between MySQL and Redis, which caused
  some private data to be disclosed to the wrong users.
- In certain fault scenarios, it could happen that two nodes both believe that
  they are the leader. This situation is called split brain, and it is
  dangerous: if both leaders accept writes, and there is no process for
  resolving conflicts, data is likely to be lost or corrupted. As a safety
  catch, some systems have a mechanism to shut down one node if two leaders are
  detected.
- What is the right timeout before the leader is declared dead? A longer timeout
  means a longer time to recovery in the case where the leader fails. However,
  if the timeout is too short, there could be unnecessary failovers. For
  example, a temporary load spike could cause a node’s response time to increase
  above the timeout, or a network glitch could cause delayed packets. If the
  system is already struggling with high load or network problems, an
  unnecessary failover is likely to make the situation worse, not better.




-- ds.h2: Implementation of Replication Logs

-- ds.h3: Statement-based replication

In the simplest case, the leader logs every write request (statement) that it
executes and sends that statement log to its followers. For a relational
database, this means that every INSERT, UPDATE, or DELETE statement is forwarded
to followers, and each follower parses and executes that SQL statement as if it
had been received from a client. Although this may sound reasonable, there are
various ways in which this approach to replication can break down:

- Nondeterministic functions like NOW() or RAND() may produce different results
  on replicas.
- Operations dependent on autoincrementing columns or existing data (e.g.,
  UPDATE ... WHERE <some condition>) must execute in the same order on all
  replicas to maintain consistency, which can be challenging with concurrent
  transactions.
- Statements with side effects (e.g., triggers, stored procedures) may produce
  different different side effects occurring on each replica unless they are
  entirely deterministic.

It is possible to work around those issues—for example, the leader can replace
any nondeterministic function calls with a fixed return value when the statement
is logged so that the followers all get the same value. However, because there
are so many edge cases, other replication methods are now generally preferred.

Statement-based replication was used in MySQL before version 5.1. It is still
sometimes used today, as it is quite compact, but by default MySQL now switches
to row-based replication (discussed shortly) if there is any nondeterminism in a
statement. VoltDB uses statement-based replication, and makes it safe by
requiring transactions to be deterministic.

-- ds.h3: Write-ahead log (WAL) shipping

- In the case of a log-structured storage engine (“SSTables and LSM-Trees”),
  this log is the main place for storage. Log segments are compacted and
  garbage-collected in the background.
- In the case of a "B-tree", which overwrites individual disk blocks, every
  modification is first written to a write-ahead log so that the index can be
  restored to a consistent state after a crash.

In either case, the log is an append-only sequence of bytes containing all
writes to the database. We can use the exact same log to build a replica on
another node.

This method of replication is used in PostgreSQL and Oracle, among others.
Despite its advantages, WAL shipping has drawbacks. It tightly couples
replication with the storage engine, making it challenging to run different
database software versions on the leader and followers. This limitation can
impact operational tasks like zero-downtime upgrades, which are feasible only if
the replication protocol supports version mismatches between leader and
followers. However, WAL shipping often requires downtime for such upgrades due
to its strict version matching requirements.

-- ds.h3: Logical (row-based) log replication

An alternative approach involves using different log formats for replication and
storage engines, allowing the replication log to be decoupled from storage
engine internals. Known as a logical log, this format differs from the physical
data representation of the storage engine.

In a relational database, a logical log typically consists of records describing
writes to database tables at the row level:

- For inserted rows, the log contains the new values of all columns.
- For deleted rows, it includes adequate information to uniquely identify the
  deleted row, often the primary key or, if unavailable, the old values of all
  columns.
- For updated rows, it includes information to uniquely identify the row and the
  new values of changed columns.

Transactions modifying multiple rows generate corresponding log records,
followed by a commit record. MySQL's binlog, configured for row-based
replication, adopts this approach.

A logical log format is also easier for external applications to parse. This
aspect is useful if you want to send the contents of a database to an external
system, such as a data warehouse for offline analysis, or for building custom
indexes and caches. This technique is called **change data capture**.

-- ds.h3: Trigger-based replication

The replication approaches described so far are implemented by the database
system, without involving any application code. You may need to move replication
up to the application layer. For example, if you want to only replicate a subset
of the data, or want to replicate from one kind of database to another, or if
you need conflict resolution logic.

Some tools, such as Oracle GoldenGate, can make data changes available to an
application by reading the database log. An alternative is to use features that
are available in many relational databases: **triggers** and **stored
procedures**.

A **trigger** lets you register custom application code that is automatically
executed when a data change (write transaction) occurs in a database system. The
trigger has the opportunity to log this change into a separate table, from which
it can be read by an external process. That external process can then apply any
necessary application logic and replicate the data change to another system.
Databus for Oracle and Bucardo for Postgres work like this, for example.

Trigger-based replication typically has greater overheads than other replication
methods, and is more prone to bugs and limitations than the database’s built-in
replication. However, it can nevertheless be useful due to its flexibility.


-- ds.h1: Problems with Replication Lag

If an application reads from an asynchronous follower, it may see out‐ dated
information if the follower has fallen behind. This inconsistency is just a
temporary state— for that reason, this effect is known as eventual consistency.

In this section we will highlight three examples of problems that are likely to
occur when there is replication lag and outline some approaches to solving them.

-- ds.h2: Reading Your Own Writes

With asynchronous replication, there is a problem, if the user views the data
shortly after making a write, the new data may not yet have reached the replica.

-- ds.image:
src: $assets.files.ddd.data-intensive-application.images.5-3.png

-- ds.h3: Implementation of Read-After-Write Consistency in Leader-Based Replication:

- Reading from Leader or Follower:
  - Read from the leader if the user might have modified the data; otherwise, read from a follower.
  - Example: User profile information editable only by the owner; hence, always read the user's own profile from the
    leader and others' profiles from a follower.
- Dynamic Criteria for Reading:
  - If most application data is editable by users, the previous approach is ineffective as most reads would need to come
    from the leader.
  - Alternative criteria for deciding whether to read from the leader can be implemented. For example, you could track
    the time of the last update and, for one minute after the last update, make all reads from the leader. You could
    also monitor the replication lag on followers and prevent queries on any follower that is more than one minute
    behind the leader.
- Utilizing Client Timestamps:
  Clients can remember the timestamp of their most recent write. The system ensures that replicas serving reads for a
  user reflect updates at least until that timestamp. If a replica is not up-to-date, reads can be directed to another
  replica or delayed until the lagging replica catches up. Timestamp can be a logical one (e.g., log sequence number) or
  the actual system clock (requires clock synchronization).
- If your replicas are distributed across multiple datacenters (for geographical proximity to users or for availability),
  there is additional complexity. Any request that needs to be served by the leader must be routed to the datacenter that
  contains the leader.

-- ds.h3: Cross-Device Read-After-Write Consistency

Users accessing the service from multiple devices (e.g., desktop web browser and mobile app). Objective is to ensure
consistency across devices, so that information entered on one device is immediately visible on others.
There are some additional issues to consider:

- Approaches relying on remembering user's last update timestamp become complex as each device's code doesn't have
  visibility into updates from other devices. Centralizing metadata becomes necessary to track updates across devices.
- If your replicas are distributed across different datacenters, there is no guarantee that connections from different
  devices will be routed to the same datacenter. (For example, if the user’s desktop computer uses the home broadband
  connection and their mobile device uses the cellular data network, the devices’ network routes may be completely
  different.) If your approach requires reading from the leader, you may first need to route requests from all of a
  user’s devices to the same datacenter.

-- ds.h2: Monotonic Reads

Our second example of an anomaly that can occur when reading from asynchronous followers is that it’s possible for a
user to see things moving backward in time.

This can happen if a user makes several reads from different replicas. For example, Figure below shows user 2345 making
the same query twice, first to a follower with little lag, then to a follower with greater lag.

-- ds.image: A user first reads from a fresh replica, then from a stale replica. Time appears to go backward. To prevent this anomaly, we need monotonic reads.
src: $assets.files.ddd.data-intensive-application.images.5-4.png

-- ds.markdown:

Monotonic reads ensure that users do not observe time going backward when making sequential reads. It guarantees that
subsequent reads by a user will not return older data after previously reading newer data. It has lesser guarantee than
strong consistency but stronger than eventual consistency.

- **Implementation**: Users consistently read from the same replica based on user ID hash.
- **Replica Selection**: User ID hash determines replica.
- **Handling Failures**: Reroute user queries to another replica if the chosen one fails.

-- ds.h2: Consistent Prefix Reads

Our third example of replication lag anomalies concerns violation of causality. Imag‐ ine the following short dialog
between Mr. Poons and Mrs. Cake:
- *Mr. Poons:* How far into the future can you see, Mrs. Cake?
- *Mrs. Cake:* About ten seconds usually, Mr. Poons.

Now, imagine a third person is listening to this conversation through followers. The things said by Mrs. Cake go through
a follower with little lag, but the things said by Mr. Poons have a longer replication lag. This observer would hear the
following:
- *Mrs. Cake:* About ten seconds usually, Mr. Poons.
- *Mr. Poons:* How far into the future can you see, Mrs. Cake?

-- ds.image: If some partitions are replicated slower than others, an observer may see the answer before they see the question.
src: $assets.files.ddd.data-intensive-application.images.5-3.png

-- ds.markdown:

This is a particular problem in partitioned (sharded) databases. In many distributed databases, different partitions
operate independently, so there is no global ordering of writes.

One solution is to make sure that any writes that are causally related to each other are written to the same partition—
but in some applications that cannot be done efficiently. There are also algorithms that explicitly keep track of causal
dependencies.

-- ds.h2: Solutions for Replication Lag

As discussed earlier, there are ways in which an application can provide a stronger guarantee than the underlying
database— for example, by performing certain kinds of reads on the leader. However, dealing with these issues in
application code is complex and easy to get wrong.

It would be better if application developers could just trust their databases to “do the right thing.” This is why
*transactions* exist.

Single-node transactions have existed for a long time. However, in the move to distributed (replicated and partitioned)
databases, many systems have abandoned them, claiming that transactions are too expensive in terms of performance and
availability, and asserting that eventual consistency is inevitable in a scalable system.



-- ds.h1: Multi-Leader Replication

-- ds.h2: Use Cases for Multi-Leader Replication

-- ds.h3: Multi-datacenter operation

In a multi-leader configuration, you can have a leader in each datacenter. Within each datacenter, regular leader–follower
replication is used; between datacenters, each datacenter’s leader replicates its changes to the leaders in other
datacenters.

-- ds.image: Multi-leader replication across multiple datacenters.
src: $assets.files.ddd.data-intensive-application.images.5-6.png

-- ds.h3: Comparison: Single-Leader vs. Multi-Leader Configurations in Multi-Datacenter Deployment

- **Performance:**
  - *Single-Leader Configuration:*
    - Every write must traverse the internet to the datacenter hosting the leader.
    - Significant latency may be added to writes.
  - *Multi-Leader Configuration:*
    - Writes can be processed locally within each datacenter.
    - Replication to other datacenters occurs asynchronously.
    - Inter-datacenter network delay is hidden from users, potentially improving perceived performance.

- **Tolerance of Datacenter Outages:**
  - *Single-Leader Configuration:*
    - Failover may promote a follower in another datacenter to be the leader if the datacenter with the leader fails.
  - *Multi-Leader Configuration:*
    - Each datacenter can operate independently.
    - Replication catches up when the failed datacenter is restored online.

- **Tolerance of Network Problems:**
  - *Single-Leader Configuration:*
    - Highly sensitive to inter-datacenter link problems as writes are made synchronously over this link.
  - *Multi-Leader Configuration:*
    - Asynchronous replication allows better tolerance to network issues.
    - Temporary network interruptions do not prevent write processing.

-- end: ds.page
