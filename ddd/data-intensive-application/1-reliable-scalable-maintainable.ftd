-- ds.page: Reliable, Scalable, and Maintainable Applications

**Note**: The content is taken from [`Designing Data Intensive Application by
Martin Kleppmann`](https://public.nikhil.io/Designing%20Data%20Intensive%20Applications.pdf)


- Reliability: The system should continue to work correctly.
- Scalable: As the system grows (in data volume, traffic volume, or complexity),
there should be reasonable ways of dealing with that growth
- Maintainability: Over time, many different people will work on the system
(engineering and operations, both maintaining current behavior and adapting the
system to new use cases), and they should all be able to work on it productively


-- ds.h1: Reliability

The system should continue to work correctly, even when things go wrong.
The things that can go wrong are called `faults`, and systems that anticipate
faults and can cope with them are called `fault-tolerant` or `resilient`.

A `fault` is not the same as a `failure`. A fault is usually defined as one
component of the system deviating from its spec, whereas a `failure` is when the
system as a whole stops providing the required service to the user. It is
impossible to reduce the probability of a fault to zero; therefore it is usually
best to design fault-tolerance mechanisms that prevent faults from causing
failures.

Many critical bugs are actually due to poor error handling; by deliberately
inducing faults, you ensure that the fault-tolerance machinery is continually
exercised and tested, which can increase your confidence that faults will be
handled correctly when they occur naturally.

- **Hardware Faults**: Add redundancy to the individual hardware components in
order to reduce the failure rate of the system. Disks may be set up in a RAID
configuration, servers may have dual power supplies and hot-swappable CPUs, and
datacenters may have batteries and diesel generators for backup power. When one
component dies, the redundant component can take its place while the broken com‐
ponent is replaced.

- **Software Faults**: Examples include:
  - A software bug,
  - A runaway process that uses up some shared resource—CPU time, memory, disk
    space, or network bandwidth.
  - A service that the system depends on that slows down, becomes unresponsive,
    or starts returning corrupted responses.
  - Cascading failures, where a small fault in one component triggers a fault in
    another component, which in turn triggers further faults

- **Human Errors**:





-- ds.h1: Scalability

Load can be described with a few numbers which we call load parameters. The best
choice of parameters depends on the architecture of your system: it may be
requests per second to a web server, the ratio of reads to writes in a database,
the number of simultaneously active users in a chat room, the hit rate on a
cache, or something else. Perhaps the average case is what matters for you, or
perhaps your bottleneck is dominated by a small number of extreme cases.


-- ds.h2: Case Study: Twitter

- Post tweet
A user can publish a new message to their followers (4.6k requests/sec on aver‐
age, over 12k requests/sec at peak).

- Home timeline
A user can view tweets posted by the people they follow (300k requests/sec).


Simply handling 12,000 writes per second (the peak rate for posting tweets) would be
fairly easy. However, Twitter’s scaling challenge is not primarily due to tweet volume,
but due to fan-out—each user follows many people, and each user is followed by
many people. There are broadly two ways of implementing these two operations:

- Posting a tweet simply inserts the new tweet into a global collection of tweets.
When a user requests their home timeline, look up all the people they follow,
find all the tweets for each of those users, and merge them (sorted by time). In a
relational database, you could write a query such as:

-- ds.code:
lang: sql

SELECT tweets.*, users.* FROM tweets
 JOIN users ON tweets.sender_id = users.id
 JOIN follows ON follows.followee_id = users.id
 WHERE follows.follower_id = current_user

-- ds.markdown:

- Maintain a cache for each user’s home timeline—like a mailbox of tweets for
each recipient user. When a user posts a tweet, look up all the people who
follow that user, and insert the new tweet into each of their home timeline
caches. The request to read the home timeline is then cheap, because its result
has been computed ahead of time.


However, the downside of approach 2 is that posting a tweet now requires a lot of
extra work. On average, a tweet is delivered to about 75 followers, so 4.6k tweets per
second become 345k writes per second to the home timeline caches. But this average
hides the fact that the number of followers per user varies wildly, and some users
have over 30 million followers. This means that a single tweet may result in over 30
million writes to home timelines! Doing this in a timely manner—Twitter tries to
deliver tweets to followers within five seconds—is a significant challenge.

The final twist of the Twitter anecdote: now that approach 2 is robustly implemented,
Twitter is moving to a hybrid of both approaches. Most users’ tweets continue to be
fanned out to home timelines at the time when they are posted, but a small number
of users with a very large number of followers (i.e., celebrities) are excepted from this
fan-out. Tweets from any celebrities that a user may follow are fetched separately and
merged with that user’s home timeline when it is read, like in approach 1. This hybrid
approach is able to deliver consistently good performance.


-- ds.h2: Describing Performance

In a batch processing system such as Hadoop, we usually care about throughput—the
number of records we can process per second, or the total time it takes to run a job
on a dataset of a certain size. In online systems, what’s usually more important is the
service’s response time — that is, the time between a client sending a request and
receiving a response.


-- ds.code: **Latency and response time**
lang: txt

Latency and response time are often used synonymously, but they
are not the same. The response time is what the client sees: besides
the actual time to process the request (the service time), it includes
network delays and queueing delays. Latency is the duration that a
request is waiting to be handled—during which it is latent, await‐
ing service


-- ds.markdown:

It’s common to see the average response time of a service reported. (Strictly speaking,
the term “average” doesn’t refer to any particular formula, but in practice it is usually
understood as the arithmetic mean: given n values, add up all the values, and divide
by n.) However, the mean is not a very good metric if you want to know your
“typical” response time, because it doesn’t tell you how many users actually
experienced that delay.

Usually it is better to use percentiles. If you take your list of response times and sort it
from fastest to slowest, then the median is the halfway point: for example, if your
median response time is 200 ms, that means half your requests return in less than
200 ms, and half your requests take longer than that.

The median is also known as the 50th percentile, and sometimes abbreviated as p50.
In order to figure out how bad your outliers are, you can look at higher percentiles:
the 95th, 99th, and 99.9th percentiles are common (abbreviated p95, p99, and p999).
They are the response time thresholds at which 95%, 99%, or 99.9% of requests are
faster than that particular threshold. For example, if the 95th percentile response time
is 1.5 seconds, that means 95 out of 100 requests take less than 1.5 seconds, and 5 out
of 100 requests take 1.5 seconds or more.

High percentiles of response times, also known as tail latencies, are important
because they directly affect users’ experience of the service. For example, Amazon
describes response time requirements for internal services in terms of the 99.9th
percentile, even though it only affects 1 in 1,000 requests. This is because the
customers with the slowest requests are often those who have the most data on
their accounts because they have made many purchases—that is, they’re the most
valuable customers. It’s important to keep those customers happy by ensuring the
website is fast for them: Amazon has also observed that a 100 ms increase in
response time reduces sales by 1%, and others report that a 1-second slowdown
reduces a customer satisfaction metric by 16%.

Percentiles are often used in service level objectives (SLOs) and service
level agreements (SLAs), contracts that define the expected performance and
availability of a service. An SLA may state that the service is considered to be
up if it has a median response time of less than 200 ms and a 99th percentile
under 1 s (if the response time is longer, it might as well be down), and the
service may be required to be up at least 99.9% of the time. These metrics set
expectations for clients of the service and allow customers to demand a refund
if the SLA is not met.

Queueing delays often account for a large part of the response time at high
percentiles. As a server can only process a small number of things in parallel
(limited, for example, by its number of CPU cores), it only takes a small number
of slow requests to hold up the processing of subsequent requests—an effect
sometimes known as **head-of-line** blocking. Even if those subsequent requests
are fast to process on the server, the client will see a slow overall response
time due to the time waiting for the prior request to complete. Due to this
effect, it is important to measure response times on the client side.


-- ds.h1: Maintainability

Three design principles for software systems:

- Operability:
Make it easy for operations teams to keep the system running smoothly.

- Simplicity:
Make it easy for new engineers to understand the system, by removing as much
complexity as possible from the system. (Note this is not the same as simplicity
of the user interface.)

- Evolvability:
Make it easy for engineers to make changes to the system in the future, adapting
it for unanticipated use cases as requirements change. Also known as
extensibility, modifiability, or plasticity.

-- end: ds.page
